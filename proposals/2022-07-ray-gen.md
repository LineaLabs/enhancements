# Summary

Ray is a popular framework for distributed execution. This proposal explores how we could use LineaPy pipelines 
to automatically leverage [Ray Workflow DAGs](https://docs.ray.io/en/master/workflows/basics.html) in a data
science script that does not already use Ray.

## General Motivation

To help data scientists leverage Ray DAGS could mean speeding up executions 
and unlocking data of new scale.

It may not be easy to use Ray, as the data scientists have to learn a new framework, and develop a mental model for when 
to use the distributed tasks and actors. If we could automate the majority of the reasoning with logic over the LineaPy pipeline, 
we could lower the barrier to entry.

## Motivating and Specifying Examples

Imagine if we have the following data science workflow

```python
def foo():
    ...
    
def bar():
    ...

def train(data):
    ...

def infer(model, data):
    ...

df1 = foo()
df2 = bar()
model = train(df1)
m2 = copy.deepcopy(model)
inference = infer(model, df2)
```

LineaPy can automatically generate a set of modules and a pipeline that connects them as follow:

### Modules
```python
def f1():
    df1 = foo()
    model = train(df1)
    return model
    
def f2():
    df2 = bar()
    return df2

def f3(model):
    m2 = copy.deepcopy(model)
    return m2

def f4(model, data):
    inference = infer(model,data)
    return inference

```

### Pipeline
```python
def pipeline():
  model = f1()
  data = f2()
  model_copy = f3(model)
  inference = f4(model, data)
```

Which would look like this

![pipeline](pipeline.jpg)

### 7/20/22 Meeting Notes
4 options for dealing with `ArtifactCollection`s in LineaPy where the artifacts in the collection share common functions, such as in the PipeLine example above:
* 2 overlapping DAGs `D1` and `D2`, one for `model_copy` and one more `inference` with all of their dependencies defined within each DAG, i.e., `f1` shows up in the definition of both `D1` and `D2`. The downside of this is that `f1` will be run twice and will not be shared across the two DAGs
* Create a NOOP sink that has `model_copy` and `inference` as parents to combine the two artifacts into a single DAG. This will ensure that `f1` is only run once, and we still get the benefit of parallel computing of `f1` and `f2`, which are independent.
* Define 3 disjoint DAGs, `D1` for `f1`, `D2` for `f2`, `f4` and the output of `D1`, and `D3` for `f3` and the output of `D1`. This will make sure that the output of `D1` is persisted to disk automatically for `D2` and `D3` to pick up and reuse. This is advantageous for LineaPy to manage intermediate results by leveraging Ray's serialization behavior.
* 2 disjoint DAGs `D1` for `model_copy` and `D2` for `inference`, where `f1` is run only within `D2` and written to disk by explicitly adding logic to persist intermediate results with an addressable name across DAGs. `D1 ` can then pick up the intermediate result for `f1` from disk. The downside of this is that for `f1` to update, we also need to run `f4` as part of `D2`, which may not be the desired behavior.

**TODO** for Linea:
* Write out concrete use cases of updating the various components at different cadences and how we envision intermediate results to be reused, annotating components that create side effects (without orchestration instructions to persist intermediate results for reuse).
* Clarify our own abtraction for executing partial DAGs for artifacts via parameterization.
